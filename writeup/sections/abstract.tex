\begin{abstract}
  Today, large language models have become immensely popular. We use one such model, BERT, for user intent classification, leveraging the Amazon Massive Intent Dataset. BERT, a state-of-the-art transformer architecture, has demonstrated remarkable performance across various natural language processing tasks. We fine-tune BERT for user intent classification and then explore advanced training techniques to enhance its performance. Additionally, we investigate the application of contrastive losses for further improvements in model accuracy. We preprocess the dataset, fine tune BERT, implement custom training techniques derived from recent research, and experimenting with contrastive learning methods. Through this process, we aim to provide insights into the efficacy of different strategies for enhancing user intent classification tasks. A variety of loss types were experimented with, but they all performed similarly. We found that refining the baseline BERT with layer-wise learning weight decay and linear scheduler warm-up yielded the best results. Our best model yielded an accuracy of 88.6\%.
\end{abstract}
