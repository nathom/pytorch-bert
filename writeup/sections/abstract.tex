\begin{abstract}
  Today, large language models have become immensely popular. We use one such model, BERT, for user intent classification, leveraging the Amazon Massive Intent Dataset. BERT, a state-of-the-art transformer architecture, has demonstrated remarkable performance across various natural language processing tasks. We fine-tune BERT for user intent classification and then explore advanced training techniques to enhance its performance. Additionally, we investigate the application of contrastive losses for further improvements in model accuracy. We preprocess the dataset, fine tune BERT, implement custom training techniques derived from recent research, and experimenting with contrastive learning methods. Through this process, we aim to provide insights into the efficacy of different strategies for enhancing user intent classification tasks. \textcolor{red}{WE NEED PERCENT CORRECT NUMBERS HERE}.
\end{abstract}
