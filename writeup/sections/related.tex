\section*{Related Works}

\section*{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}
In \cite{bert} the authors introduce a groundbreaking model in natural language processing that utilize the concept of bidirectional context representation learning. By pretraining a transformer-based neural network on large corpora with masked language modeling and next sentence prediction tasks, BERT is able to capture deep contextual information from both left and right contexts of a word. This bidirectional understanding of text enables BERT to achieve state-of-the-art performance on a wide range of NLP tasks, including but not limited to question answering, sentiment analysis, named entity recognition, and machine translation.

\section*{Supervised Contrastive Learning}
In \cite{scl} the authors introduce supervised contrastive learning, a method for learning representations of data by maximizing agreement between similar pairs and minimizing agreement between dissimilar pairs. Supervised contrastive learning leverages labeled data to guide the learning process, making it particularly suitable for tasks where labeled examples are abundant. The method has shown promising results in various applications, including image classification, natural language processing, and speech recognition.

\section*{SimCSE: Simple Contrastive Learning of Sentence Embeddings}
In \cite{simcse}, the authors propose a simple yet effective approach for learning sentence embeddings through contrastive learning. By formulating the contrastive objective in a straightforward manner, SimCSE achieves competitive performance on various downstream tasks without requiring complex architectures or extensive hyperparameter tuning. The method has been shown to learn semantically meaningful representations of sentences, making it valuable for tasks such as semantic textual similarity, text classification, and paraphrase detection.
